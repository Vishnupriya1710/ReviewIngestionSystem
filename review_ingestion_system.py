# -*- coding: utf-8 -*-
"""review-ingestion-system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18N8iLSpEF6JxCy1eT7vzbKDWwALUXidQ
"""

# pip install pyspark

import argparse
import sys
import os

if 'google.colab' in sys.modules:
  inappropriate_words = "/content/data/inappropriate_words.txt"
  input = "/content/data/reviews.jsonl"
  output = "/content/output/"
  aggregations = "/content/output/aggregatedReviews/"
else:
  parser = argparse.ArgumentParser()
  parser.add_argument("--input", default="data\\reviews.jsonl")
  parser.add_argument("--output", default="output\\")
  parser.add_argument("--inappropriate_words", default="data\\inappropriate_words.txt")
  parser.add_argument("--aggregations", default="output\\aggregatedReviews\\")

  args = parser.parse_args()

  inappropriate_words = args.inappropriate_words
  input = args.input
  output = args.output
  aggregations = args.aggregations

  os.makedirs(output, exist_ok=True)
  os.makedirs(aggregations, exist_ok=True)

# from awsglue.transforms import *
# from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
# from awsglue.context import GlueContext
# from awsglue.job import Job
import json
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, TimestampType
from pyspark.sql.functions import *
from datetime import datetime

# sc = SparkContext.getOrCreate()
# glueContext = GlueContext(sc)
# spark = glueContext.spark_session.builder.appName("ReviewIngestion").config("spark.sql.legacy.timeParserPolicy", "LEGACY").getOrCreate()
# job = Job(glueContext)

# Create the sparksession
spark = SparkSession.builder.appName("ReviewIngestion").master("local[*]").config("spark.sql.legacy.timeParserPolicy", "LEGACY").getOrCreate()

from logging import exception
"""
    Load the JSONL file to a Dataframe and return the dataframe
    Input:
    - Input path location
    Output:
    - Dataframe created
"""
def loadJSONtoDf(input_path):
  try:
    #review_df = spark.read.schema(schema).json(reviewFilePath)
    review_df = spark.read.json(input_path)
    return review_df
  except Exception as e:
        print(f"An exception occurred in loadJSONtoDf(): {e}")

"""
    Reads a text file from the specified S3 path and returns these words as a list.
    Input:
    - s3_text_file_path
    Output:
    - List of inappropriate words
"""
def getInappropriateWords(s3_text_file_path):
  try:
    inappropriate_words_df = spark.read.text(s3_text_file_path)
    return inappropriate_words_df.rdd.flatMap(lambda x: x).collect()
  except Exception as e:
    print(f"An exception occurred in getInappropriateWords(): {e}")

"""
    Converts a JSON schema into a Spark DataFrame schema.
    Input:
    - JSON content as a string defining the schema.
    Output:
    - Spark StructType representing the schema.
"""
def load_schema_from_json(json_content):
  try:
    schema_json = json.loads(json_content)

    fields = []
    for field in schema_json['properties']:
        name = field
        dtype = schema_json['properties'][field]['type']
        nullable = field not in schema_json['required']

        if dtype == "string":
            data_type = StringType()
        elif dtype == "integer":
            data_type = IntegerType()
        elif dtype == "number":
            data_type = DoubleType()
        elif dtype == "string" and schema_json['properties'][field].get("format") == "date-time":
            data_type = TimestampType()
        else:
            raise ValueError(f"Unsupported data type: {dtype}")

        fields.append(StructField(name, data_type, nullable))

    return StructType(fields)
  except Exception as e:
      print(f"An exception occurred in load_schema_from_json(): {e}")

"""
    Cleans the input dataframe by removing the NULL columns.
    Input:
    - Input DataFrame read from the input file.
    Output:
    - Cleaned dataframe with only non-null values.
"""
def dataCleaning(review_df):
  try:
    filtered_review_df = review_df.filter(review_df["restaurantId"].isNotNull() &
                                          review_df["reviewId"].isNotNull() &
                                          review_df["text"].isNotNull() &
                                          review_df["rating"].isNotNull() &
                                          review_df["publishedAt"].isNotNull())
    return filtered_review_df
  except Exception as e:
    print(f"An exception occurred in dataCleaning(): {e}")

"""
    Converts the 'publishedAt' column in a DataFrame to a timestamp using multiple date formats.
    Input:
    - Input DataFrame containing the 'publishedAt' column as a string.
    Output:
    - DataFrame with 'publishedAt' column converted to a timestamp.
"""
def getDateFormattedDf(df):
  try:
    date_formats = ["yyyy-MM-dd'T'HH:mm:ss.SSSX",  # ISO 8601 with timezone
                "yyyy-MM-dd",                 # Date without time
                "yyyy-MM-dd HH:mm:ss",        # Date with time
                "yyyy/MM/dd"]                 # Alternative date format
    return filtered_review_df.withColumn("publishedAt", coalesce(*[to_timestamp(col("publishedAt"), f) for f in date_formats]))
  except Exception as e:
    print(f"An exception occurred in getDateFormattedDf(): {e}")

"""
    Filters the input DataFrame into two DataFrames based on the 'publishedAt' date.
    Input:
    - Input DataFrame containing the 'publishedAt' column as a timestamp.
    Output:
    - outdated_filtered (DataFrame) containing records with 'publishedAt' within the last three years.
    - discarded_outdated (DataFrame) containing records with 'publishedAt' older than three years.
"""
def getOutdatedFilteredDf(df):
  try:
    three_years_ago = date_sub(current_date(), 3 * 365)
    outdated_filtered = filtered_review_df.filter(col("publishedAt") >= three_years_ago)
    discarded_outdated = filtered_review_df.filter(col("publishedAt") < three_years_ago)
    return outdated_filtered, discarded_outdated
  except Exception as e:
    print(f"An exception occurred in getOutdatedFilteredDf(): {e}")

"""
    Filters the DataFrame based on the proportion of inappropriate words in the text.
    Input:
    - Input DataFrame with reviews containing a 'text' column.
    Output:
    - threshold_reviews (DataFrame) with reviews where the proportion of inappropriate words is less than 20%.
    - discarded_threshold (DataFrame) with reviews where the proportion of inappropriate words is 20% or more.
"""
def getProportionFilteredDf(outdated_filtered):
  try:
    proportion_filtered = outdated_filtered.withColumn("cleaned_text", regexp_replace(col("text"), r"[^a-zA-Z0-9\s]", ""))
    proportion_filtered = proportion_filtered.withColumn("words", split(col("cleaned_text"), "\\s+")).withColumn("total_words", size(col("words"))).drop("words")

    # Marked the inappropriate words as INAPPROPRIATEWORD
    for word in inappropriate_word_list:
        pattern = f"(?i)\\b{word}\\w*\\b"
        proportion_filtered = proportion_filtered.withColumn("cleaned_text", regexp_replace(col("cleaned_text"), pattern, "INAPPROPRIATEWORD"))

    counted_proportion_filtered = proportion_filtered.withColumn("inapproppriate_word_count", size(split(col("cleaned_text"), "INAPPROPRIATEWORD")) - 1)
    counted_proportion_filtered = counted_proportion_filtered.withColumn("proportion",col("inapproppriate_word_count") / col("total_words"))

#     Applied the threshold value
    threshold_reviews = counted_proportion_filtered.filter(col("proportion")<.2).drop("inapproppriate_word_count","total_words")
    discarded_threshold = counted_proportion_filtered.filter(col("proportion")>=.2).drop("inapproppriate_word_count","total_words","cleaned_text","proportion")

    return threshold_reviews, discarded_threshold
  except Exception as e:
    print(f"An exception occurred in getProportionFilteredDf(): {e}")

"""
    Processes the DataFrame to replace inappropriate words in the text with "****" and writes the final DataFrame to a JSONL file.
    Input:
    - threshold_reviews (DataFrame): Input DataFrame with filtered reviews.
    Output:
    - The final processed DataFrame after removing unnecessary columns and writing to a file.

"""
def writeFinalProcessedReviews(threshold_reviews):
  try:
    for word in inappropriate_word_list:
        pattern = f"(?i)\\b{word}\\w*\\b"
        threshold_reviews = threshold_reviews.withColumn("text", regexp_replace(col("text"), pattern, "****"))
    # threshold_reviews.show(50)
    finalProcessed = threshold_reviews.drop("text_word_count","words","total_words","cleaned_text","inapp words count","proportion")

    if not (finalProcessed is None or finalProcessed.isEmpty()):
      # Get current date and time
      now = datetime.now()
      datePrefix = str(now.year)+"/"+str(now.month)+"/"+str(now.day)+"/"
      # print(datePrefix)

      finalProcessedReviewsPath = output+"processedReviews/"+datePrefix
      path = finalProcessedReviewsPath

      # Write dataframe to a JSON file
      finalProcessed.write.mode("overwrite").json('data.jsonl')

      # Merge JSON files into a single file
      finalProcessed.coalesce(1).write.mode("overwrite").json(path)
    return finalProcessed
  except Exception as e:
    print(f"An exception occurred in writeFinalProcessedReviews(): {e}")

"""
    Aggregates various metrics from the final processed reviews DataFrame.
    Input:
    - finalProcessed (DataFrame): The final processed DataFrame containing review details.
    Output:
    - transformed_df (DataFrame) with aggregated metrics for each restaurant.
"""
def aggregationsReview(finalProcessed):
  try:
    # Convert publishedAt to timestamp
    finalProcessed = finalProcessed.withColumn("publishedAt", col("publishedAt").cast("timestamp"))

    # Calculate the length of each review
    finalProcessed = finalProcessed.withColumn("review_length", length(col("text")))

    # Calculate review age in days
    finalProcessed = finalProcessed.withColumn("review_age_days", datediff(current_date(), col("publishedAt")))

    # Aggregate metrics
    finalProcessed = finalProcessed.repartition("restaurantId")
    aggregations_df = finalProcessed.groupBy("restaurantId").agg(
        count("*").alias("reviewCount"),
        round(avg("rating"), 2).alias("averageRating"),
        round(avg("review_length"), 2).alias("averageReviewLength"),
        min("review_age_days").alias("newest_reviewAge"),
        max("review_age_days").alias("oldest_reviewAge"),
        avg("review_age_days").alias("average_reviewAge")
    ).orderBy("restaurantId")

    transformed_df = aggregations_df.select(
        col("restaurantId"),
        col("reviewCount"),
        col("averageRating"),
        col("averageReviewLength"),
        struct(
            col("oldest_reviewAge").alias("oldest"),
            col("newest_reviewAge").alias("newest"),
            col("average_reviewAge").cast("integer").alias("average")
        ).alias("reviewAge")
    )
    # transformed_df.printSchema()
    return transformed_df
  except Exception as e:
    print(f"An exception occurred in aggregationsReview(): {e}")

"""
    Writes the transformed DataFrame with aggregated metrics to a JSONL file.
    Input:
    - transformed_df (DataFrame): The DataFrame containing aggregated metrics for each restaurant.
    - output_path (str): The output path where the final JSONL file will be saved.
"""
def writeAggregations(transformed_df):
  try:
    # path = "/content/output/aggregatedReviews/"
    # Write dataframe to a JSON file
    transformed_df.write.mode("overwrite").json('aggregated_data.jsonl')

    if not (transformed_df is None or transformed_df.isEmpty()):
      # Get current date and time
      now = datetime.now()
      datePrefix = str(now.year)+"/"+str(now.month)+"/"+str(now.day)+"/"
      transformed_df.coalesce(1).write.mode("overwrite").json(aggregations+datePrefix)
  except Exception as e:
    print(f"An exception occurred in writeAggregations(): {e}")

def writeDiscarded(*dfs):
  try:
    valid_dfs = [df for df in dfs if df is not None]
    if not valid_dfs:
        return None
    discardedReviews = valid_dfs[0]
    for df in valid_dfs[1:]:
        discardedReviews = discardedReviews.union(df)

    if not (discardedReviews is None or discardedReviews.isEmpty()):
      # Get current date and time
      now = datetime.now()
      datePrefix = str(now.year)+"/"+str(now.month)+"/"+str(now.day)+"/"
      path = output+"/discardedReviews/"+datePrefix

      # Write dataframe to a JSON file
      discardedReviews.write.mode("overwrite").json('discarded_data.jsonl')

      # Merge JSON files into a single file
      discardedReviews.coalesce(1).write.mode("overwrite").json(path)
  except Exception as e:
    print(f"An exception occurred in writeDiscarded(): {e}")

if __name__ == "__main__":
  try:
    inappropriate_word_list = getInappropriateWords(inappropriate_words)
    # print(inappropriate_word_list)

    """Reading the 'reviews.jsonl' input file"""
    review_df = loadJSONtoDf(input)
    if review_df is None or review_df.isEmpty():
      raise Exception("review Dataframe not created!")

      """Data cleaning"""
    review_df.cache()
    filtered_review_df = dataCleaning(review_df)

    if filtered_review_df is None:
      raise Exception("There are no records after filtering")

    filtered_review_df.cache()

    """Change the date format to a standardized format"""
    filtered_review_df = getDateFormattedDf(filtered_review_df)

    """Remove the outdated reviews"""
    outdated_filtered, discarded_outdated = getOutdatedFilteredDf(filtered_review_df)

    if outdated_filtered is None:
      raise Exception("All records are older than 3 years")

    """Filter by proportion"""
    threshold_reviews, discarded_threshold = getProportionFilteredDf(outdated_filtered)

    if threshold_reviews is None:
      raise Exception("All reviews are inappropriate")

    """Write final processed reviews"""
    finalProcessed = writeFinalProcessedReviews(threshold_reviews)

    if finalProcessed is None:
      raise Exception("There are no final processed reviews")

    """Aggregate the processed reviews"""
    transformed_df = aggregationsReview(finalProcessed)

    if transformed_df is None:
      raise Exception("No records to aggregate")

    """Write aggregations"""
    writeAggregations(transformed_df)

    """Write the discarded reviews"""
    writeDiscarded(discarded_outdated,discarded_threshold)

      # transformed_df.show()
  except Exception as e:
    print(f"An exception occurred: {e}")

